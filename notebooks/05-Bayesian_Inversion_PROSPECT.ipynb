{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Introduction\" data-toc-modified-id=\"Introduction-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Introduction</a></span></li><li><span><a href=\"#Visualising-some-data-from-the-LOPEX'93-dataset\" data-toc-modified-id=\"Visualising-some-data-from-the-LOPEX'93-dataset-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Visualising some data from the LOPEX'93 dataset</a></span><ul class=\"toc-item\"><li><span><a href=\"#Comments\" data-toc-modified-id=\"Comments-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Comments</a></span></li></ul></li><li><span><a href=\"#Model-inversion\" data-toc-modified-id=\"Model-inversion-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Model inversion</a></span><ul class=\"toc-item\"><li><span><a href=\"#Simple-PROSPECT-inversion\" data-toc-modified-id=\"Simple-PROSPECT-inversion-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Simple PROSPECT inversion</a></span><ul class=\"toc-item\"><li><span><a href=\"#Some-exercises...\" data-toc-modified-id=\"Some-exercises...-3.1.1\"><span class=\"toc-item-num\">3.1.1&nbsp;&nbsp;</span>Some exercises...</a></span></li></ul></li></ul></li><li><span><a href=\"#Final-remarks\" data-toc-modified-id=\"Final-remarks-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Final remarks</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"float:right\">\n",
    "    <table>\n",
    "    <tr>\n",
    "        <td> <img src=\"../figs/pangeos-small-1.png\" alt=\"PANGEOS\" style=\"width:200px;height:45px;\"/> \n",
    "        <td> <img src=\"../figs/kcl_logo.png\" alt=\"King's College London\" style=\"width:54px;height:40px;\"/> \n",
    "        <td> <img src=\"../figs/nceo_logo.png\" alt=\"NCEO\" style=\"width:200px;height:40px;\"/> \n",
    "        <td> <img src=\"../figs/multiply_logo.png\" alt=\"H2020 Multiply\" style=\"width:40px;height:40px;\"/>\n",
    "    </tr>\n",
    "    </table>\n",
    "</div>\n",
    "&nbsp;\n",
    "\n",
    "# Bayesian inversion of the PROSPECT model. The role of uncertainty\n",
    "\n",
    "**Author:** Jose GÃ³mez-Dans (NCEO & UCL)  `jose.gomez-dans@kcl.ac.uk`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Introduction\n",
    "\n",
    "The previous notebook explored using a simple function fitting approach to retrieve biophysical parameters from leaf reflectance and transmittance spectra. In this notebook, we'll extend the previous work by considering the effect of **uncertainty** in the retrieval. This is a crucial aspect of any retrieval, as it allows us to quantify the confidence we have in the retrieved parameters.\n",
    "\n",
    "Earlier, we only used a simple concept of uncertainty: we did a set of random starts for the minimisation, and looked at how the results changed. This is only a part of the uncertainty in the retrieval. We can probably think of the following sources of uncertainty:\n",
    "* **Measurement uncertainty**: this is the uncertainty in the measurements themselves.\n",
    "* **Model uncertainty**: this is the uncertainty in the model itself. In our case, we used the PROSPECT model, which is a simple model that is known to have some limitations. \n",
    "* **Retrieval uncertainty**: this is the uncertainty in the retrieval process itself. In the previous notebook, this is the uncertainty that arises from starting on different points in the parameter space, but for other methods it might be to do with limitations in the inversion algorithm.\n",
    "* **Representation uncertainty** In many cases, we use models that describe whole leaf and/or canopy dynamics, and only sample a small part of the variance in the real world.\n",
    "* **Prior uncertainty**: all retrieval methods constrain the parameter space in some way or another. You can think of this as *prior information*. In the previous notebook, we used a uniform prior, but in many cases, we might have more information about the parameters we are trying to retrieve. How we encode that information will affect the retrieval.\n",
    "\n",
    "\n",
    "We will first start by looking at the LOPEX'93 data again...\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "    This notebook has a number of commented out cells. These are exercises for you to try out. You can uncomment them by removing the `#` at the beginning of the line.\n",
    "</div>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from pangeos_uq.prosail_funcs import (\n",
    "    read_lopex_sample,\n",
    "    the_cost_function_covariance,\n",
    "    optimise_random_starts,\n",
    "    calculate_mean_and_covariance,\n",
    ")\n",
    "\n",
    "from pangeos_uq.mcmc import generate_samples\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "import ipywidgets as widgets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualising uncertainty from the LOPEX'93 dataset\n",
    "\n",
    "The [LOPEX'93 dataset](https://data.ecosis.org/dataset/13aef0ce-dd6f-4b35-91d9-28932e506c41/resource/4029b5d3-2b84-46e3-8fd8-c801d86cf6f1/download/leaf-optical-properties-experiment-93-lopex93.pdf) contains five replicates per sample. We will use this to estimate the uncertainty in the measurements. \n",
    "\n",
    "We will assume that the measurements are corrupted by additive zero mean Gaussian noise. For reflectance, this can be written as:\n",
    "$$\n",
    "\\vec{\\rho}_{\\text{meas}} = \\vec{\\rho}_{\\text{true}} + \\vec{\\epsilon}(0, \\mathcal{C}_{obs}),\n",
    "$$\n",
    "\n",
    "where $\\vec{\\rho}_{\\text{meas}}$ is the observed reflectance for all wavelengths, $\\vec{\\rho}_{\\text{true}}$ is the true reflectance, and $\\vec{\\epsilon}(0, \\mathcal{C}_{obs})$ is a zero mean Gaussian noise with covariance $\\mathcal{C}_{obs}$. A similar expression holds for transmittance.\n",
    "\n",
    "In the next couple of cells, you'll select some samples from the LOPEX'93 database, and we'll use them to calculate the uncertainty in the measurements. We'll visualise the combined reflectance and transmittance **correlation matrix**. This is just a scaled version of the covariance matrix, and it tells us how the different bands and measurements are correlated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "widgets.interact(\n",
    "    read_lopex_sample, sample_no=widgets.IntSlider(min=1, max=116, value=23)\n",
    ")\n",
    "refl, trans = read_lopex_sample.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_rho, mean_tau, cov_matrix, inv_cov_matrix, correlation_matrix = (\n",
    "    calculate_mean_and_covariance(refl, trans)\n",
    ")\n",
    "\n",
    "plt.imshow(\n",
    "    correlation_matrix,\n",
    "    interpolation=\"nearest\",\n",
    "    cmap=\"inferno\",\n",
    "    vmin=-1,\n",
    "    vmax=1,\n",
    ")\n",
    "plt.axvline(2101, color=\"g\", lw=2)\n",
    "plt.axhline(2101, color=\"g\", lw=2)\n",
    "plt.colorbar(shrink=0.5);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The correlation matrix can be interpted using the green horizontal and vertical lines that divide into 4 quadrants. The top left quadrant is the correlation between the reflectance bands, the bottom right quadrant is the correlation between the transmittance bands, and the other two quadrants are the correlation between reflectance and transmittance bands. The main diagonal is just 1s, as it's the correlation of a band with itself. But we can see some very clear patterns in the correlation matrix.\n",
    "\n",
    "Can you try to explain the patterns you see in the correlation matrix?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model inversion\n",
    "\n",
    "We will now invert the model using a normal log-likelihood function. The log-likelihood function is given by:\n",
    "\n",
    "$$\n",
    "J(\\mathbf{x}) = \\frac{1}{2} \\left( M(\\mathbf{x}) - \\vec{y} \\right)^{\\top} \\mathcal{C}^{-1} \\left( M(\\mathbf{x}) - \\vec{y} \\right).\n",
    "$$\n",
    "\n",
    "In essence, we'll be trying to see the effect of the uncertainty in the retrievals. For the retrievals, we'll use a time-consuming MCMC approach, that has the benefit of being simple to follow and to implement. The MCMC approach will allow us to sample the posterior distribution of the parameters, and to quantify the uncertainty in the retrieval.\n",
    "\n",
    "We'll run three experiments:\n",
    "1. A simple inversion with fixed uncertainty and no spectral correlation (e.g. the covariance matrix is diagonal with a fixed value).\n",
    "2. As above, but with the diagonal now reflecting the uncertainty in the measurements.\n",
    "3. As above, but with the full covariance matrix reflecting the uncertainty in the measurements, including spectral correlation.\n",
    "\n",
    "We will run this with reflectance, but you're welcome to extend this to transmittance too.\n",
    "\n",
    "We will define a few common variables that will be used in the inversion. You can change things around a bit, but the main parameter to change is the `n_iterations` variable, which controls the number of samples in the MCMC chain. The more samples, the better the estimate of the posterior distribution, but the longer the code will take to run. Values of less than 1000 are probably not very useful, and values of more than 10000 probably take too long to run.\n",
    "\n",
    "Since we're mostly interested in looking at the effect of uncertainty, we can probably just start by using our gradient descent method to find the maximum likelihood estimate of the parameters. This will give us a good starting point for the MCMC chain, and ensure that we focus on where the posterior distribution is likely to be.\n",
    "\n",
    "For reference, on my laptop, 5000 samples take about 1 minute to run.\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "    Some of the code here will take time to run slowly if you set `n_iterations` to a large number. You can test how long this will take on your computer by running the code with a small number of iterations first (say 1000) and extending it to a more sensible value once you know how long it'll take.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lobound = np.array([0.8, 0, 0, 0, 0.0043, 0.0017])\n",
    "hibound = np.array([2.8, 80, 20, 1, 0.0439, 0.0152])\n",
    "\n",
    "# Get a starting point for the MCMC close to where the action happens\n",
    "\n",
    "df, _, _ = optimise_random_starts(\n",
    "    mean_rho,\n",
    "    None,\n",
    "    n_tries=5,\n",
    "    lobound=lobound,\n",
    "    hibound=hibound,\n",
    "    verbose=False,\n",
    "    do_plots=False,\n",
    ")\n",
    "# Extract the average solution values. Discard last column as that's cost\n",
    "initial_value = df.iloc[-2, :-1].values\n",
    "# Need to get a reasonable proposal step... divide std dev by 10 to\n",
    "# get to the right scale(ish)\n",
    "transitions = df.iloc[-1:, :-1].values / 10.0\n",
    "\n",
    "n_iterations = 10_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this_inv_cov_matrix = np.eye(4202) * (np.diagonal(inv_cov_matrix).mean())\n",
    "# cost_simple = []\n",
    "\n",
    "\n",
    "# def logpdf(x: np.ndarray) -> float:\n",
    "#     \"\"\"Log likelihood function for the MCMC sampler. This function takes up\n",
    "#     some variables from the main namespace, so I don't have to boether about\n",
    "#     adding them as arguments to the function. This is a bit of a hack, but\n",
    "#     there we go\"\"\"\n",
    "#     lobound = np.array([0.8, 0, 0, 0, 0.0043, 0.0017])\n",
    "#     hibound = np.ndarray = np.array([2.8, 80, 20, 1, 0.0439, 0.0152])\n",
    "\n",
    "#     if np.any(x < lobound) or np.any(x > hibound):\n",
    "#         # Proposed value is outside the bounds, never accept\n",
    "#         return -np.inf\n",
    "#     # Uncomment below to get different measurements into the inversion\n",
    "#     # Using only reflectance measurements\n",
    "#     cost = the_cost_function_covariance(x, mean_rho, None, this_inv_cov_matrix)\n",
    "#     # Using **both** reflectance and transmittance measurements\n",
    "#     # cost = the_cost_function_covariance(x, mean_rho,\n",
    "#     #               mean_trans, this_inv_cov_matrix)\n",
    "#     # Using **only** transmittance measurements\n",
    "#     # cost = the_cost_function_covariance(x, mean_rho,\n",
    "#     #               None, this_inv_cov_matrix)\n",
    "#     cost_simple.append(cost)\n",
    "#     return cost\n",
    "\n",
    "\n",
    "# vals_simple = np.array(\n",
    "#     list(\n",
    "#         generate_samples(\n",
    "#             initial_value, n_iterations, logpdf, scaling=transitions\n",
    "#         )\n",
    "#     )\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this_inv_cov_matrix = np.eye(4202) * inv_cov_matrix.diagonal()\n",
    "\n",
    "# cost_diag = []\n",
    "\n",
    "\n",
    "# def logpdf(x: np.ndarray) -> float:\n",
    "#     \"\"\"Log likelihood function for the MCMC sampler. This function takes up\n",
    "#     some variables from the main namespace, so I don't have to boether about\n",
    "#     adding them as arguments to the function. This is a bit of a hack, but\n",
    "#     there we go\"\"\"\n",
    "#     lobound = np.array([0.8, 0, 0, 0, 0.0043, 0.0017])\n",
    "#     hibound = np.ndarray = np.array([2.8, 80, 20, 1, 0.0439, 0.0152])\n",
    "\n",
    "#     if np.any(x < lobound) or np.any(x > hibound):\n",
    "#         # Proposed value is outside the bounds, never accept\n",
    "#         return -np.inf\n",
    "#     # Uncomment below to get different measurements into the inversion\n",
    "#     # Using only reflectance measurements\n",
    "#     cost = the_cost_function_covariance(x, mean_rho, None, this_inv_cov_matrix)\n",
    "#     # Using **both** reflectance and transmittance measurements\n",
    "#     # cost = the_cost_function_covariance(x, mean_rho,\n",
    "#     #               mean_trans, this_inv_cov_matrix)\n",
    "#     # Using **only** transmittance measurements\n",
    "#     # cost = the_cost_function_covariance(x, mean_rho,\n",
    "#     #               None, this_inv_cov_matrix)\n",
    "\n",
    "#     cost_diag.append(cost)\n",
    "#     return cost\n",
    "\n",
    "\n",
    "# vals_diag = np.array(\n",
    "#     list(\n",
    "#         generate_samples(\n",
    "#             initial_value, n_iterations, logpdf, scaling=transitions\n",
    "#         )\n",
    "#     )\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this_inv_cov_matrix = inv_cov_matrix\n",
    "\n",
    "# cost_full = []\n",
    "\n",
    "\n",
    "# def logpdf(x: np.ndarray) -> float:\n",
    "#     \"\"\"Log likelihood function for the MCMC sampler. This function takes up\n",
    "#     some variables from the main namespace, so I don't have to boether about\n",
    "#     adding them as arguments to the function. This is a bit of a hack, but\n",
    "#     there we go\"\"\"\n",
    "#     lobound = np.array([0.8, 0, 0, 0, 0.0043, 0.0017])\n",
    "#     hibound = np.ndarray = np.array([2.8, 80, 20, 1, 0.0439, 0.0152])\n",
    "\n",
    "#     if np.any(x < lobound) or np.any(x > hibound):\n",
    "#         # Proposed value is outside the bounds, never accept\n",
    "#         return -np.inf\n",
    "#     # Uncomment below to get different measurements into the inversion\n",
    "#     # Using only reflectance measurements\n",
    "#     cost = the_cost_function_covariance(x, mean_rho, None, this_inv_cov_matrix)\n",
    "#     # Using **both** reflectance and transmittance measurements\n",
    "#     # cost = the_cost_function_covariance(x, mean_rho,\n",
    "#     #               mean_trans, this_inv_cov_matrix)\n",
    "#     # Using **only** transmittance measurements\n",
    "#     # cost = the_cost_function_covariance(x, mean_rho,\n",
    "#     #               None, this_inv_cov_matrix)\n",
    "#     cost_full.append(cost)\n",
    "#     return cost\n",
    "\n",
    "\n",
    "# vals_full = np.array(\n",
    "#     list(\n",
    "#         generate_samples(\n",
    "#             initial_value, n_iterations, logpdf, scaling=transitions\n",
    "#         )\n",
    "#     )\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now run the MCMC sampler. In a real life MCMC sampling problem, we would have to run the sampler for a long time to ensure convergence. We would also have to check for convergence, and to ensure that the samples are independent. We would also have to check that the samples are representative of the posterior distribution.\n",
    "\n",
    "Since this isn't real life, we'll just take the last 1000 samples from the chain and assume they're representative of the posterior distribution. We'll then plot the marginal posterior distributions of the parameters, and the joint posterior distributions of the parameters.\n",
    "\n",
    "You can save your sampling results below. We also provide a sample file with the results of the sampling for a large number of iterations. You can use this to plot the results without having to run the MCMC sampler, or to compare things."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save your inversion results to an npz file\n",
    "# np.savez(\n",
    "#     \"prospect_mcmc_example_YOURNAME.npz\",\n",
    "#     vals_simple=vals_simple,\n",
    "#     vals_diag=vals_diag,\n",
    "#     vals_full=vals_full,\n",
    "#     cost_simple=cost_simple,\n",
    "#     cost_diag=cost_diag,\n",
    "#     cost_full=cost_full,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Retrieve the reference MCMC samples from disk\n",
    "f = np.load(\"prospect_mcmc_example_NEW.npz\")\n",
    "vals_simple_reference = f[\"vals_simple\"][-5000:, :]\n",
    "vals_diag_reference = f[\"vals_diag\"][-5000:, :]\n",
    "vals_full_reference = f[\"vals_full\"][-5000:, :]\n",
    "cost_simple_reference = f[\"cost_simple\"]\n",
    "cost_diag_reference = f[\"cost_diag\"]\n",
    "cost_full_reference = f[\"cost_full\"]\n",
    "\n",
    "# vals_simple_reference = vals_simple\n",
    "# vals_diag_reference = vals_diag\n",
    "# vals_full_reference = vals_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Packs the reference samples into pandas DataFrames for easy plotting\n",
    "\n",
    "df_simple = pd.DataFrame(\n",
    "    vals_simple_reference, columns=[\"n\", \"cab\", \"car\", \"cbrown\", \"cw\", \"cm\"]\n",
    ")\n",
    "df_diag = pd.DataFrame(\n",
    "    vals_diag_reference, columns=[\"n\", \"cab\", \"car\", \"cbrown\", \"cw\", \"cm\"]\n",
    ")\n",
    "\n",
    "df_full = pd.DataFrame(\n",
    "    vals_full_reference, columns=[\"n\", \"cab\", \"car\", \"cbrown\", \"cw\", \"cm\"]\n",
    ")\n",
    "\n",
    "df_simple[\"source\"] = \"Simple\"\n",
    "df_diag[\"source\"] = \"Diagonal\"\n",
    "df_full[\"source\"] = \"Full\"\n",
    "\n",
    "df_combined = pd.concat([df_simple, df_diag, df_full])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pairplot of the MCMC samples. Take every 25th sample to speed up\n",
    "# plotting and to deal with serial correlation in the MCMC samples.\n",
    "\n",
    "#sns.pairplot(df_combined.iloc[::25, :], hue=\"source\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Do a simple parameter spread plot\n",
    "\n",
    "# # # Create a figure with 2x3 subplots\n",
    "# fig, axes = plt.subplots(3, 2, figsize=(8, 8))\n",
    "\n",
    "# # Flatten axes array for easier indexing\n",
    "# axes = axes.flatten()\n",
    "\n",
    "# # Plot violin plots for each variable\n",
    "# for i, var in enumerate(df_simple.columns[:-1]):\n",
    "#     #sns.stripplot(x='source', y=var, data=df_combined, ax=axes[i])\n",
    "#     sns.violinplot(\n",
    "#         x=\"source\",\n",
    "#         y=var,\n",
    "#         data=df_combined,\n",
    "#         ax=axes[i],\n",
    "#         hue=\"source\"\n",
    "# )\n",
    "#     axes[i].set_title(var)\n",
    "# fig.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plots take a considerable amount of time to run, so you might want to skip this step, and plot some previously prepared samples. Here's a pair plot of the samples, and a plot of the posterior distribution of the parameters.\n",
    "![Pair plot](../figs/mcmc_pairplot.png)\n",
    "\n",
    "\n",
    "![Sample distribution](../figs/mcmc_prospect.png)\n",
    "\n",
    "\n",
    "### Try this plots with your data!\n",
    "\n",
    "You can run the sampler above with your data, and plot the results. There are some commented out bits and pieces to give you hints on how to do this. Some suggestions:\n",
    "1. Run with reflectance, and then with transmittance, and finally, reflectance and transmittance together.\n",
    "2. Play around with the different inverse covariance matrices\n",
    "3. Try different priors for the parameters. Currently, it's just a bounded space (uniform \"prior\"), but you could use a Gaussian prior, or a mixture of Gaussians, or a Student's t distribution, etc.\n",
    "\n",
    "For example, assume that you wanted to have a Gaussian prior with a mean vector $\\mu$, and prior covariances $\\sigma_{cab}, \\dots$. You would then have to modify the log-likelihood function to include the prior. Note that it is often a good idea to still keep the boundaries on the parameters, so as not to run prospect with meaningless parameters (e.g. $N < 0$).\n",
    "\n",
    "```python\n",
    "# assume you have a prior mean and covariance vectors\n",
    "mu = np.array([2, 40, 20, 0.5, 0.01, 0.01]) # for example\n",
    "# The covariance matrix is a diagonal matrix with the prior variances \n",
    "# given by whatever you want. Here I double the mean value and square it.\n",
    "cov = np.eye(6) * (mu*2)**2 # for example\n",
    "prior = scipy.stats.multivariate_normal(mu, cov)\n",
    "# Define this inside `logpdf`, and then add it to the log-likelihood:\n",
    "return cost + prob.logpdf(x)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc/4.0/\"><img alt=\"Creative Commons Licence\" style=\"border-width:0\" src=\"https://i.creativecommons.org/l/by-nc/4.0/88x31.png\" /></a><br />This work is licensed under a <a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc/4.0/\">Creative Commons Attribution-NonCommercial 4.0 International License</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
